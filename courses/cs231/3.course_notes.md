## Parameterized mapping from images to label scores
$$ \text{training dataset of images  } x_i\in\\{\R}^D $$
Here,  i=1…N and yi=1...K. In CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10, since there are 10 distinct classes (dog, cat, car, etc). 

Linear classifier. In this module we will start out with arguably the simplest possible function, a linear mapping:
$$ f(x_i, W, b) = Wx_i + b $$

x_i = flattened out to a single column vector of shape [D x 1].<br>
W(Weight) = size [K x D]<br>
b(bias) = size [K x 1]<br>

<b>Note:</b>
1. Notice also that we think of the input data (xi,yi) as given and fixed, but we have control over the setting of the parameters W,b. Our goal will be to set these in such way that the computed scores match the ground truth labels across the whole training set.
2. An advantage of this approach is that the training data is used to learn the parameters W,b, but once the learning is complete we can discard the entire training set and only keep the learned parameters. That is because a new test image can be simply forwarded through the function and classified based on the computed scores.

## Interpreting a linear classifier:

![imagemap](images/imagemap.jpg)

An example of mapping an image to class scores. For the sake of visualization, we assume the image only has 4 pixels (4 monochrome pixels, we are not considering color channels in this example for brevity), and that we have 3 classes (red (cat), green (dog), blue (ship) class). (Clarification: in particular, the colors here simply indicate 3 classes and are not related to the RGB channels.) We stretch the image pixels into a column and perform matrix multiplication to get the scores for each class. Note that this particular set of weights W is not good at all: the weights assign our cat image a very low cat score. In particular, this set of weights seems convinced that it's looking at a dog.

```python
import numpy as np
W = np.array([[0.2, -0.5, 0.1, 2.0], [1.5, 1.3, 2.1, 0.0], [0.0, 0.25, 0.2, -0.3] ])
x_i = np.array([56, 231, 24, 2])
b = np.array([1.1, 3.2, -1.2])
out = np.dot(W,x_i) + b
```
```bash
output:
array([-96.8 , 437.9 ,  60.75])
```

Image data preprocessing - Further common preprocessing is to scale each input feature so that its values range from [-1, 1].

### Loss Function or Cost Function
we saw that we don’t have control over the data (xi,yi) (it is fixed and given), but we do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data.

Simply, A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The cost function has its own curve and its own gradients. The slope of this curve tells us how to update our parameters to make the model more accurate.

![LossFunction](images/Loss.png)

That labels values can be 0 to 1 integer.
