### Epoch vs Batch Size vs Iterations
<b>Epoch:</b></br>
One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.

<b>Batch Size:</b></br>
Total number of training examples present in a single batch. We canâ€™t pass the entire dataset into the neural net at once. So, you divide dataset into Number of <b>Batches</b> or <b>sets</b> or <b>parts.</b>

<b>Iterations:</b></br> 
Iterations is the number of batches needed to complete one epoch.

For example,</br> 
We can divide the dataset of 2000 examples into batches of 500 then it will take a 4 iterations to complete 1 epoch.